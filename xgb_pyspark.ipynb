{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using xgboost on pyspark\n",
    "xgb is a wild used model on the industry, unlike other algos like random forest which is build-in in pyspark library, we need to set up by ourselves. But should not be so difficult.\n",
    "\n",
    "## 1. prepare the jars & wrappers\n",
    "\n",
    "we need to download two jars and one python wrapper. the link is listed below:\n",
    "- xgboost4j: https://mvnrepository.com/artifact/ml.dmlc/xgboost4j/0.72?fireglass_rsn=true\n",
    "- xgboost4j-spark: https://mvnrepository.com/artifact/ml.dmlc/xgboost4j-spark/0.72?fireglass_rsn=true\n",
    "- XGBoost python wrapper: https://link.zhihu.com/?target=https%3A//github.com/dmlc/xgboost/files/2161553/sparkxgb.zip%3Ffireglass_rsn%3Dtrue\n",
    "\n",
    "and i will use bank data as an example:\n",
    "dataset: https://link.zhihu.com/?target=https%3A//www.kaggle.com/janiobachmann/bank-marketing-dataset\n",
    "\n",
    "import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars xgboost4j-spark-0.72.jar,xgboost4j-0.72.jar pyspark-shell'\n",
    "import findspark\n",
    "findspark.init()\n",
    " \n",
    "import pyspark\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col\n",
    " \n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"PySpark XGBOOST\")\\\n",
    "        .master(\"local[*]\")\\\n",
    "        .getOrCreate()\n",
    " \n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "spark.sparkContext.addPyFile(\"sparkxgb.zip\")\n",
    "from sparkxgb import XGBoostEstimator\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load spark and revise the column names to make sure there is no '.' on the columns, because spark dont accept\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = spark\\\n",
    "  .read\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .csv(\"bank.csv\")\n",
    "\n",
    "tran_tab = str.maketrans({x:None for x in list('{()}')})\n",
    "df_all = df_all.toDF(*(re.sub(r'[\\.\\s]+', '_', c).translate(tran_tab) for c in df_all.columns))\n",
    " \n",
    "# fill na\n",
    "df_all = df_all.na.fill(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when training model we need to set up pipeline first. In pipeline, the stage are defined to show the operation sequence.\n",
    "Moreover, when we deal with some categorial features,we need to do some transformtion on these features. Here we use stringindex & OneHotEncoder to do the transfermation, and for numerical features we can just add it directly as stages into pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unused_col = ['day','month']\n",
    "df_all = df_all.select([col for col in df_all.columns if col not in unused_col])\n",
    "numeric_features = [t[0] for t in df_all.dtypes if t[1] == 'int']\n",
    "cols = df_all.columns\n",
    " \n",
    " \n",
    "string_col = [t[0] for t in df_all.dtypes if t[1] != 'int']\n",
    "string_col = [x for x in string_col if x!='deposit']\n",
    " \n",
    "for S in string_col:\n",
    "    globals()[str(S)+'Indexer'] = StringIndexer()\\\n",
    "                                  .setInputCol(S)\\\n",
    "                                  .setOutputCol(str(S)+'Index')\\\n",
    "                                  .setHandleInvalid(\"keep\")\n",
    "    globals()[str(S)+'classVecIndexer'] = OneHotEncoderEstimator(inputCols=[globals()[str(S)+'Indexer'].getOutputCol()], outputCols=[str(S)+ \"classVec\"]) \n",
    " \n",
    " \n",
    "# zip to one 'feature' columns\n",
    "feature_col = [s+'Index' for s in string_col]\n",
    "feature_col.extend([str(s)+ \"classVec\"  for s in string_col])\n",
    "feature_col.extend(numeric_features)\n",
    " \n",
    "vectorAssembler = VectorAssembler()\\\n",
    "  .setInputCols(feature_col)\\\n",
    "  .setOutputCol(\"features\")\n",
    "  \n",
    "# index label columns\n",
    "label_stringIdx = StringIndexer(inputCol = 'deposit', outputCol = 'label')\n",
    " \n",
    " \n",
    "# define xgboost\n",
    "xgboost = XGBoostEstimator(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=\"label\", \n",
    "    predictionCol=\"prediction\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. define pipeline and all the stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_stage = [globals()[str(S)+'Indexer'] for S in string_col]\n",
    "feat_stage.extend([globals()[str(s)+ \"classVecIndexer\"]  for s in string_col])\n",
    "feat_stage.extend([vectorAssembler,label_stringIdx,xgboost])\n",
    "xgb_pipeline = Pipeline().setStages(feat_stage)\n",
    " \n",
    "# split train & test\n",
    "trainDF, testDF = df_all.randomSplit([0.8, 0.2], seed=24)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. model training & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "model = xgb_pipeline.fit(trainDF)\n",
    "# predict \n",
    "pre   = model.transform(testDF)\\\n",
    "        .select(col(\"label\"),col('probabilities'),col(\"prediction\"))\n",
    "\n",
    "# to pandas df \n",
    "cx = pre.toPandas()\n",
    "cx[\"probabilities\"] =   cx[\"probabilities\"].apply(lambda x: x.values)\n",
    "cx[['prob_0','prob_1']] = pd.DataFrame(cx.probabilities.tolist(), index= cx.index)\n",
    "cx  = cx[[\"label\",'prob_1']].sort_values(by = ['prob_1'],ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate\n",
    "metrics.roc_auc_score(cx.label, cx.prob_1)\n",
    " \n",
    "# plot ROC curve\n",
    "y_pred_proba =cx.prob_1\n",
    "fpr, tpr, _ = metrics.roc_curve(cx.label,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(cx.label, y_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
